{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce790dcd",
   "metadata": {},
   "source": [
    "## 2과목 데이터 처리 기술의 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a13a7",
   "metadata": {},
   "source": [
    "### 데이터처리 프로세스\n",
    "#### 1. ETL(Extraction, Transformation and Load)\n",
    "원천데이터 추출부터 ODS, DW, DM 등으로의 적재까지 관련된 데이터 이동 및 변환 절차이다.<br><br>\n",
    "**인터페이스단계**: 원천데이터로부터 데이터를 획득하는 단계<br>$\\rightarrow$ **스테이징 단계**: 인터페이스단계에서 획득한 데이터를 일대일 또는 일대다로 스테이징 테이블에 저장하는 단계<br>$\\rightarrow$ **프로파일링 단계**: 통계, 데이터 마이닝 등으로 데이터 품질을 점검하는 단계<br>$\\rightarrow$ **클렌징 단계**: 클렌징 ETL 프로세스들로 프로파일링 단계에서 식별된 이상치$\\cdot$결측치 등 수정, 변환, 표준화, 비지니스룰 적용 단계<br>$\\rightarrow$ **인티그레이션 단계**: 클렌징한 데이터들을 단일 통합 테이블에 적재하는 단계<br>$\\rightarrow$ **비정규화 단계&익스포트 단계**: 운영보고서 작성, DW, DM 적재를 위해 데이터 비정규화를 수행하고 적재하는 단계\n",
    "  \n",
    "• 전통적인 ETL 기술은 DW 구성만을 주목적으로 하였으나 최근 ODS, BI플랫폼, 하둡 등 다양한 통합 메커니즘들을 지원하는 것으로 확장하고 있다.<br>\n",
    "•  빅데이터 시대의 도래로 대용량 비정형 데이터 수집과 정형 데이터로의 변환이 중시되어 Hadoop, Hadoop Ecosystem 같은 데이터처리 빅데이터 기술이<br>$~$ 활발히 활용된다.\n",
    "- **`Hadoop`**: 분산 병렬 처리 MapReduce 시스템과 분산 파일시스템 HDFS를 핵심 구성요소로 가지는 플랫폼 기술\n",
    "    - 분산 서버수 제한이 없으며 비공유 분산 아키텍처로 선형적인 성능과 용량 확장(초고속 수집 성능과 확장성, 인터페이스 상속), <br>HDFS의 데이터 3중복제로 고장감내성(안정적인 데이터 전송), MapReduce 작동에 의해 비지니스 로직에만 집중할 수 있는 장점을 갖는다. \n",
    "    - MapReduce를 보완하며 분산 어플리케이션을 구현하기 위한 자원 관리 프레임워크를 지원하는 YARN을  Hadoop 2.0부터 제공한다.\n",
    "-  **`Hadoop Ecosystem`**: 다양한 비지니스 요구를 지원하기 위한 Hadoop 기반 시스템들\n",
    "    - 데이터수집: Chukwa(HDFS에 안정적으로 저장시키는 플랫폼), Scribe(페이스북, 수집데이터를 중앙집중서버로 전송해 처리하는 플랫폼)<br>예시)아파치 Flume-NG$\\cdot$페이스북 Scribe$\\cdot$아파치 Chukwa(대용량 로그데이터 수집)\n",
    "    - 데이터 연동: Sqoop(다양한 저장소에 대용량 데이터를 신속하게 전송하는 솔루션)\n",
    "    - 대용량 SQL 질의: Pig(복잡한 MapReduce 프로그래밍을 대체할 Pig Latin 언어 제공기술), <br>\n",
    "      $~~~~~~~~~~~~~~~~~~~~~~~~~~$ Hive(테이블 단위의 데이터 저장, HiveQL 쿼리를 지원하는 Hadoop 기반의 DW 기술)\n",
    "    - 실시간 SQL 질의: Impala(클라우데라), Tajo(고려대 대학원)\n",
    "- SQL on 하둡 기술:드릴(MapR), 스팅거(호튼웍스), 샤크(Hive와 호환되는 인메모리 기반 대용량 DW시스템), 호크(피보탈), <br>\n",
    "  $~~~~~~~~~~~~~~~~~~~~~~~~~~~$ 프레스토(페이스북의 데이터웨어하우징 엔진)\n",
    "  \n",
    "#### 2. CDC(Change Data Capture)\n",
    "데이터베이스 내 데이터(원천데이터)에 대한 변경을 식별해 필요한 후속처리를 자동화하는 기술, 설계 기법, 구조이다.\n",
    "- Timestamp/Version/Status on Rows: 세 기법을 모두 활용한 기법\n",
    "  - 정교한 쿼리 생성에 활용해 개발 유연성을 제공한다.\n",
    "- Event Programming: 데이터 변경 식별 기능을 어플리케이션을 구현하는 방법\n",
    "  - 어플리케이션 개발 부담과 복잡도가 증가하나 다양한 조건의 CDC 메커니즘을 구현할 수 있다.\n",
    "- Log Scanner on Database: DBMS에서 제공하는 트랜잭션 로그에 대해 스캐닝, 변경내역을 해석해 변경을 CDC 메커니즘을 구현하는 기법\n",
    "  - 데이터베이스와 어플리케이션에 대한 영향도 최소화, 변경 식별 지연시간 최소화, 트랜잭션 무결성에 대한 영향도 최소화, 데이터베이스 스키마 변경이 불필요하다.\n",
    "  \n",
    "#### 3. EAI(Enterprise Application Integration)\n",
    "비지니스 프로세스를 중심으로 기업 내/간 데이터를 연계해 각종 어플리케이션 간 상호연동(상호융화, 동기화)이 가능하도록 통합하는 솔루션이다.\n",
    "- Spoke에 해당하는 다수 정보 시스템 데이터를 중앙의 Hub가 중계$\\cdot$연계해 노드 간 연결 개수 및 구조를 단순화하는 Hub and Spoke 방식   으로 데이터를 연계한다.\n",
    "- 미들웨어(Hub)를 이용하여 비지니스 로직을 중심으로 Application을 프로세스 및 메세지 차원에서 통합$\\cdot$연계$\\cdot$관리한다.\n",
    "- 구현유형\n",
    "  - Mediation(Publish/subscribe): EAI 엔진이 중개자(Broker)로 동작하여 특정 이벤트 발생을 식별해 약속된 정보시스템에 데이터를 전달하는 구조\n",
    "  - Federation(Request/reply): EAI 엔진이 외부 정보 시스템으로부터 데이터 요청들을 일괄적으로 수령해 필요한 데이터를 전달하는 구조\n",
    "- 정보 시스템 개발 및 유지 보수 비용 절감, 기업 정보 시스템의 지속적 발전 기반 확보, 협력사$\\cdot$파트너$\\cdot$고객과의 상호협력 프로세스 연계, 웹 서비스 등 인터넷 비지니스를 위한 기본 토대 확립, 지역적으로 분리되어 있는 정보 시스템들 간의 데이터 동기화, 그룹 및 지주 회사 계열사들 간 상호 관련 데이터 동기화 등을 위한 데이터 표준화 기반 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af3b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
